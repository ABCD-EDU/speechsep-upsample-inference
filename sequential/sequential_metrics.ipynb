{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arian\\AppData\\Local\\Temp\\ipykernel_26872\\1103696278.py:7: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from speechbrain.pretrained import SepformerSeparation as separator\n",
    "import soundfile\n",
    "import torchaudio\n",
    "from pydub import AudioSegment\n",
    "from IPython.display import Audio \n",
    "from IPython.core.display import display\n",
    "import os\n",
    "from scipy.io import wavfile\n",
    "import noisereduce as nr\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import speechbrain as sb\n",
    "import speechbrain.nnet.schedulers as schedulers\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "import csv\n",
    "device = torch.device('cuda')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# from mir_eval.separation import bss_eval_sources\n",
    "from speechbrain.dataio.dataio import read_audio\n",
    "from fast_bss_eval import bss_eval_sources\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import concurrent.futures\n",
    "def parse_audio(path):\n",
    "   audio,sr = torchaudio.load(path)\n",
    "   audio = torchaudio.functional.resample(audio, sr, 8000)\n",
    "   # audio, sr = librosa.load(path,sr=8000)\n",
    "   # audio = torch.from_numpy(audio).unsqueeze(0)\n",
    "\n",
    "   # audio = audio\n",
    "   # print(audio.shape)\n",
    "   \n",
    "   return audio\n",
    "def append_audio(item_name, path, out_est, out_target, out_est_up, out_mix):\n",
    "   # source1_path = f\"{path}/{item_name}_source1hat_up.wav\"\n",
    "   # source2_path = f\"{path}/{item_name}_source2hat_up.wav\"\n",
    "   # if not os.path.isfile(source1_path) or not os.path.isfile(source2_path):\n",
    "   #    print(source1_path)\n",
    "   #    return\n",
    "\n",
    "   # Orig\n",
    "   source1_path = f\"{path}/{item_name}_source1.wav\"\n",
    "   source2_path = f\"{path}/{item_name}_source2.wav\"\n",
    "   out_target[item_name] = [parse_audio(source1_path), parse_audio(source2_path)]\n",
    "\n",
    "   source1_path = f\"{path}/{item_name}_source1hat.wav\"\n",
    "   source2_path = f\"{path}/{item_name}_source2hat.wav\"\n",
    "   out_est[item_name] = [parse_audio(source1_path), parse_audio(source2_path)]\n",
    "   \n",
    "   # source1_path = f\"{path}/{item_name}_source1hat_up.wav\"\n",
    "   # source2_path = f\"{path}/{item_name}_source2hat_up.wav\"\n",
    "   # out_est_up[item_name] = [parse_audio(source1_path), parse_audio(source2_path)]\n",
    "   # print(out_est_up[item_name])\n",
    "\n",
    "   mix_path = f\"{path}/{item_name}_mix.wav\"\n",
    "   out_mix[item_name]=(parse_audio(mix_path))\n",
    "   \n",
    "\n",
    "def read_all_audio(path, upsampled=False):\n",
    "   \n",
    "   audio_ids = []\n",
    "   out_target= {}\n",
    "   out_est = {}\n",
    "   out_est_upsampled = {}\n",
    "   out_mix = {}\n",
    "   all_files = os.listdir(path)\n",
    "   item_names = [file_name.split('_')[0] for file_name in all_files] \n",
    "   item_names = list(set(item_names))\n",
    "   \n",
    "   # with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "   for item_name in tqdm(item_names, total=len(item_names)):\n",
    "      append_audio(item_name, path, out_est, out_target, out_est_upsampled, out_mix)\n",
    "      \n",
    "   audio_ids =list(sorted(out_target.keys())) \n",
    "   out_target = list(dict(sorted(out_target.items())).values())\n",
    "   out_est= list(dict(sorted(out_est.items())).values())\n",
    "   out_est_upsampled= list(dict(sorted(out_est_upsampled.items())).values())\n",
    "   out_mix= list(dict(sorted(out_mix.items())).values())\n",
    "   print(audio_ids, out_target, out_est, out_est_upsampled)\n",
    "   return audio_ids, out_target, out_est,out_est_upsampled, out_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Separation(sb.Brain):\n",
    "   def compute_objectives(self, predictions, targets):\n",
    "        \"\"\"Computes the si-snr loss\"\"\"\n",
    "        return self.hparams.loss(targets, predictions)\n",
    "     \n",
    "   def get_metrics(self,audio_ids, targets, preds, mixtures,output_path):\n",
    "\n",
    "        # Create folders where to store audio\n",
    "      # save_file = os.path.join(output_path, \"test_results.csv\")\n",
    "      save_file = output_path\n",
    "\n",
    "        # Variable init\n",
    "\n",
    "      all_sdrs = []\n",
    "      all_sdrs_i = []\n",
    "      all_sisnrs = []\n",
    "      all_sisnrs_i = []\n",
    "      csv_columns = [\"snt_id\", \"sdr\", \"sdr_i\", \"si-snr\", \"si-snr_i\"]\n",
    "\n",
    "        \n",
    "      \n",
    "      with open(save_file, \"w\") as results_csv:\n",
    "         writer = csv.DictWriter(results_csv, fieldnames=csv_columns)\n",
    "         writer.writeheader()\n",
    "            \n",
    "         for audio_id, target, pred,mixture_ in tqdm(zip(audio_ids,targets, preds,mixtures), total=len(targets)):\n",
    "            target = torch.cat(\n",
    "            [target[i].unsqueeze(-1) for i in range(self.hparams.num_spks)],\n",
    "            dim=-1,\n",
    "        ).to(self.device)\n",
    "            \n",
    "            pred = torch.cat(\n",
    "            [pred[i].unsqueeze(-1) for i in range(self.hparams.num_spks)],\n",
    "            dim=-1,\n",
    "        ).to(self.device)\n",
    "            \n",
    "  \n",
    "            sisnr = self.compute_objectives(pred, target)\n",
    "            # COmpute SI-SNR Improvement\n",
    "            mixture_signal = torch.stack(\n",
    "               [mixture_] * self.hparams.num_spks, dim=-1\n",
    "            )\n",
    "            \n",
    "            arrs = [pred[0], target[0], mixture_signal[0]]\n",
    "            for item in arrs:\n",
    "               temp_item = (item!=np.inf).all() and (item!=np.NINF).all()\n",
    "               if not temp_item:\n",
    "                  continue\n",
    "            # if mixture_signal[0] == np.inf or mixture_signal[1] ==np.inf:\n",
    "            \n",
    "            mixture_signal = mixture_signal.to(target.device)\n",
    "            sisnr_baseline = self.compute_objectives(\n",
    "               mixture_signal, target\n",
    "            )\n",
    "            sisnr_i = sisnr.cpu().numpy() - sisnr_baseline.cpu().numpy()\n",
    "  # Compute SDR\n",
    "            sdr, _, _, _ = bss_eval_sources(\n",
    "               target[0].t(),\n",
    "               pred[0].t(),\n",
    "            )\n",
    "\n",
    "            sdr_baseline, _, _, _ = bss_eval_sources(\n",
    "               target[0].t(),\n",
    "               mixture_signal[0].t(),\n",
    "            )\n",
    "            \n",
    "            sdr = sdr.cpu().numpy()\n",
    "            sdr_baseline = sdr_baseline.cpu().numpy()\n",
    "           \n",
    "            sdr_i = sdr.mean() - sdr_baseline.mean()\n",
    "\n",
    "            # Saving on a csv file\n",
    "            row = {\n",
    "               \"snt_id\": audio_id,\n",
    "               \"sdr\": sdr.mean(),\n",
    "               \"sdr_i\": sdr_i,\n",
    "               \"si-snr\": -sisnr.item(),\n",
    "               \"si-snr_i\": -sisnr_i.item(),\n",
    "            }\n",
    "            writer.writerow(row)\n",
    "\n",
    "            # Metric Accumulation\n",
    "            all_sdrs.append(sdr.mean())\n",
    "            all_sdrs_i.append(sdr_i.mean())\n",
    "            all_sisnrs.append(-sisnr.item())\n",
    "            all_sisnrs_i.append(-sisnr_i.item())\n",
    "      # logger.info(\"Mean SISNR is {}\".format(np.array(all_sisnrs).mean()))\n",
    "      # logger.info(\"Mean SISNRi is {}\".format(np.array(all_sisnrs_i).mean()))\n",
    "      # logger.info(\"Mean SDR is {}\".format(np.array(all_sdrs).mean()))\n",
    "      # logger.info(\"Mean SDRi is {}\".format(np.array(all_sdrs_i).mean()))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading standard_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 52.64it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with standard_model | upsamle: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load hyperparameters file with command-line overrides\n",
    "hparams_file, run_opts, overrides = sb.parse_arguments([\"hyperparams.yaml\"])\n",
    "hparams_file = 'hyperparams.yaml'\n",
    "#    run_opts = {\"device\": \"cuda:0\"}\n",
    "with open(hparams_file) as fin:\n",
    "   hparams = load_hyperpyyaml(fin, overrides)\n",
    "   \n",
    "# Load pretrained model if pretrained_separator is present in the yaml\n",
    "if \"pretrained_separator\" in hparams:\n",
    "   # run_on_main(hparams[\"pretrained_separator\"].collect_files)\n",
    "   hparams[\"pretrained_separator\"].load_collected(\n",
    "      device=run_opts[\"device\"]\n",
    "   )\n",
    "# Brain class initialization\n",
    "separator = Separation(\n",
    "   modules=hparams[\"modules\"],\n",
    "   run_opts={\"device\": \"cuda\"},\n",
    "   hparams=hparams,\n",
    ")\n",
    "\n",
    "configs = [\"standard_model\"]\n",
    "# configs = [\"w_noise_speedperturb\",\"w_noise_wavedrop\"]\n",
    "# configs = [\"no_noise_speedperturb\"]\n",
    "UPSAMPLE= [False]\n",
    "# UPSAMPLE= [True]\n",
    "curr_aud =[]\n",
    "for config in configs:\n",
    "   print(f\"Reading {config}\")\n",
    "   # out = read_all_audio(f'results/{config}/audio_results')\n",
    "   # audio_ids,out_target, out_est, out_est_upsampled, out_mix = out\n",
    "   # audio_ids,out_target, out_est, out_est_upsampled, out_mix = read_all_audio(f'results/{config}/audio_results')\n",
    "   audio_ids,out_target, out_est, out_est_upsampled, out_mix = read_all_audio(f'./separated')\n",
    "   \n",
    "   \n",
    "   for is_upsample in UPSAMPLE:\n",
    "      if is_upsample:\n",
    "         out_target = out_est\n",
    "         out_est = out_est_upsampled   \n",
    "      # upsample = \"upsampled\" if upsample else \"not_upsampled\"\n",
    "      output_path = f\"test.csv\"\n",
    "      separator.get_metrics(audio_ids, out_target, out_est, out_mix, output_path=output_path)\n",
    "      \n",
    "      print(f\"Done with {config} | upsamle: {is_upsample}\")\n",
    "# output_path = f\"results/epoch_30/true_test_result.csv\"\n",
    "# separator.get_metrics(audio_ids, out_target, out_est, out_mix, output_path=output_path)\n",
    "# separator.get_metrics(audio_ids, out_est, out_est_upsampled, out_mix, output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = separator.from_hparams(source=\"speechbrain/sepformer-libri2mix\", savedir=f'./models/standard_model',run_opts={\"device\":\"cuda\"})\n",
    "# est_sources = models[model_name].separate_file(path=f'{mix_path}/{file}') \n",
    "      \n",
    "# item_name = file.split(\"_\")[0]\n",
    "# display(Audio(f\"{mix_path}/{file}\"))\n",
    "# audio1_filename = f\"{output_path}/{item_name}_source1hat.wav\"\n",
    "# audio2_filename = f\"{output_path}/{item_name}_source2hat.wav\"\n",
    "\n",
    "# torchaudio.save(audio1_filename, est_sources[:, :, 0].detach().cpu(), 8000)\n",
    "# torchaudio.save(audio2_filename, est_sources[:, :, 1].detach().cpu(), 8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
