{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arian\\AppData\\Local\\Temp\\ipykernel_24756\\2507175996.py:7: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from speechbrain.pretrained import SepformerSeparation as separator\n",
    "import soundfile\n",
    "import torchaudio\n",
    "from pydub import AudioSegment\n",
    "from IPython.display import Audio \n",
    "from IPython.core.display import display\n",
    "import os\n",
    "from scipy.io import wavfile\n",
    "import shutil\n",
    "import noisereduce as nr\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sota_model= separator.from_hparams(source=\"speechbrain/sepformer-wsj02mix\", savedir=f'pretrained_models/sepformer-wsj02mix',run_opts={\"device\":\"cuda\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_names = [\"no_noise_speedperturb\",\"w_noise_speedperturb\",\"w_noise_wavedrop\", \"standard_model\"]\n",
    "model_names = [\"standard_model\",\"w_noise_wavedrop\"]\n",
    "models = {}\n",
    "for model_name in model_names:\n",
    "   models[model_name]= separator.from_hparams(source=\"speechbrain/sepformer-libri2mix\", savedir=f'models/{model_name}',run_opts={\"device\":\"cuda\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s]C:\\Users\\Arian\\AppData\\Local\\Temp\\ipykernel_24756\\1578545387.py:25: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  rate1, audio1 = wavfile.read(audio1_filename)\n",
      "C:\\Users\\Arian\\AppData\\Local\\Temp\\ipykernel_24756\\1578545387.py:26: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  rate2, audio2 = wavfile.read(audio2_filename)\n",
      "  5%|â–         | 145/3000 [07:02<2:18:45,  2.92s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m audio1 \u001b[39m=\u001b[39m AudioSegment\u001b[39m.\u001b[39mfrom_file(audio1_filename)\n\u001b[0;32m     34\u001b[0m audio1 \u001b[39m=\u001b[39m audio1\u001b[39m+\u001b[39m\u001b[39m6\u001b[39m\n\u001b[1;32m---> 35\u001b[0m audio2 \u001b[39m=\u001b[39m AudioSegment\u001b[39m.\u001b[39;49mfrom_file(audio2_filename)\n\u001b[0;32m     36\u001b[0m audio2 \u001b[39m=\u001b[39m audio2\u001b[39m+\u001b[39m\u001b[39m6\u001b[39m\n\u001b[0;32m     38\u001b[0m mixed_audio \u001b[39m=\u001b[39m audio1\u001b[39m.\u001b[39moverlay(audio2)\n",
      "File \u001b[1;32mc:\\Users\\Arian\\.conda\\envs\\inference\\lib\\site-packages\\pydub\\audio_segment.py:768\u001b[0m, in \u001b[0;36mAudioSegment.from_file\u001b[1;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001b[0m\n\u001b[0;32m    764\u001b[0m log_conversion(conversion_command)\n\u001b[0;32m    766\u001b[0m p \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39mPopen(conversion_command, stdin\u001b[39m=\u001b[39mstdin_parameter,\n\u001b[0;32m    767\u001b[0m                      stdout\u001b[39m=\u001b[39msubprocess\u001b[39m.\u001b[39mPIPE, stderr\u001b[39m=\u001b[39msubprocess\u001b[39m.\u001b[39mPIPE)\n\u001b[1;32m--> 768\u001b[0m p_out, p_err \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39;49mcommunicate(\u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mstdin_data)\n\u001b[0;32m    770\u001b[0m \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mreturncode \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(p_out) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    771\u001b[0m     \u001b[39mif\u001b[39;00m close_file:\n",
      "File \u001b[1;32mc:\\Users\\Arian\\.conda\\envs\\inference\\lib\\subprocess.py:1028\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[1;34m(self, input, timeout)\u001b[0m\n\u001b[0;32m   1025\u001b[0m     endtime \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1028\u001b[0m     stdout, stderr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_communicate(\u001b[39minput\u001b[39;49m, endtime, timeout)\n\u001b[0;32m   1029\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1030\u001b[0m     \u001b[39m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m     \u001b[39m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Arian\\.conda\\envs\\inference\\lib\\subprocess.py:1399\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[1;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[0;32m   1395\u001b[0m \u001b[39m# Wait for the reader threads, or time out.  If we time out, the\u001b[39;00m\n\u001b[0;32m   1396\u001b[0m \u001b[39m# threads remain reading and the fds left open in case the user\u001b[39;00m\n\u001b[0;32m   1397\u001b[0m \u001b[39m# calls communicate again.\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1399\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstdout_thread\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_remaining_time(endtime))\n\u001b[0;32m   1400\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout_thread\u001b[39m.\u001b[39mis_alive():\n\u001b[0;32m   1401\u001b[0m         \u001b[39mraise\u001b[39;00m TimeoutExpired(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, orig_timeout)\n",
      "File \u001b[1;32mc:\\Users\\Arian\\.conda\\envs\\inference\\lib\\threading.py:1011\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1008\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1010\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1011\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[0;32m   1012\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1013\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1014\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Arian\\.conda\\envs\\inference\\lib\\threading.py:1027\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[39mif\u001b[39;00m lock \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# already determined that the C code is done\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_stopped\n\u001b[1;32m-> 1027\u001b[0m \u001b[39melif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[0;32m   1028\u001b[0m     lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m   1029\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_path = 'mix'\n",
    "output_path = 'audio_results'\n",
    "\n",
    "file_names = os.listdir('mix')\n",
    "for i,file in tqdm(enumerate(file_names), total=len(file_names)):\n",
    "   for model_name in model_names:\n",
    "      mix_path = \"mix\"\n",
    "      mix_audio_path = file\n",
    "      if model_name !=model_names[0]:\n",
    "         mix_path = \"synthetic_mix\"\n",
    "         mix_audio_path = f\"item{i}_mix.wav\"\n",
    "      # print(f'{mix_path}/{mix_audio_path}')\n",
    "      est_sources = models[model_name].separate_file(path=f'{mix_path}/{mix_audio_path}') \n",
    "      item_name = f'item{i}'\n",
    "      # shutil.copy2(f'mix/{item_name}_mix.wav',f'{mix_path}/{mix_audio_path}',) \n",
    "      # item_name = file.split(\"_\")[0]\n",
    "      # display(Audio(f\"{mix_path}/{file}\"))\n",
    "      audio1_filename = f\"{output_path}/{item_name}_source1hat.wav\"\n",
    "      audio2_filename = f\"{output_path}/{item_name}_source2hat.wav\"\n",
    "\n",
    "      torchaudio.save(audio1_filename, est_sources[:, :, 0].detach().cpu(), 8000)\n",
    "      torchaudio.save(audio2_filename, est_sources[:, :, 1].detach().cpu(), 8000)\n",
    "      \n",
    "     \n",
    "      rate1, audio1 = wavfile.read(audio1_filename)\n",
    "      rate2, audio2 = wavfile.read(audio2_filename)\n",
    "      audio1 = nr.reduce_noise(y=audio1, sr=8000)\n",
    "      audio2 = nr.reduce_noise(y=audio2, sr=8000)\n",
    "      \n",
    "      wavfile.write(audio1_filename,8000, audio1)\n",
    "      wavfile.write(audio2_filename,8000, audio2)\n",
    "      \n",
    "      audio1 = AudioSegment.from_file(audio1_filename)\n",
    "      audio1 = audio1+6\n",
    "      audio2 = AudioSegment.from_file(audio2_filename)\n",
    "      audio2 = audio2+6\n",
    "      \n",
    "      mixed_audio = audio1.overlay(audio2)\n",
    "      \n",
    "      # mixed_audio.export(f\"synthetic_mix/{model_name}_{item_name}_mix.wav\",format=\"wav\")\n",
    "      \n",
    "      mixed_audio.export(f\"synthetic_mix\\{item_name}_mix.wav\",format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling the audio from 48000 Hz to 8000 Hz\n"
     ]
    }
   ],
   "source": [
    "# model = separator.from_hparams(source=\"speechbrain/sepformer-libri2mix\", savedir=f'pretrained/sepformer-wsj02mix',run_opts={\"device\":\"cuda\"})\n",
    "# est_sources = models[model_name].separate_file(path=f'{mix_path}/{file}') \n",
    "      \n",
    "# item_name = file.split(\"_\")[0]\n",
    "\n",
    "# audio1_filename = f\"{output_path}/{item_name}_source1.wav\"\n",
    "# audio2_filename = f\"{output_path}/{item_name}_source2.wav\"\n",
    "\n",
    "# torchaudio.save(audio1_filename, est_sources[:, :, 0].detach().cpu(), 8000)\n",
    "# torchaudio.save(audio2_filename, est_sources[:, :, 1].detach().cpu(), 8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
