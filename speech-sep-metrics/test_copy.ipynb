{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import speechbrain as sb\n",
    "import speechbrain.nnet.schedulers as schedulers\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "import csv\n",
    "device = torch.device('cuda')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# from mir_eval.separation import bss_eval_sources\n",
    "from speechbrain.dataio.dataio import read_audio\n",
    "from fast_bss_eval import bss_eval_sources\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import concurrent.futures\n",
    "def parse_audio(path):\n",
    "   audio,sr = torchaudio.load(path)\n",
    "   audio = torchaudio.functional.resample(audio, sr, 8000)\n",
    "   # audio, sr = librosa.load(path,sr=8000)\n",
    "   # audio = torch.from_numpy(audio).unsqueeze(0)\n",
    "\n",
    "   # audio = audio\n",
    "   # print(audio.shape)\n",
    "   \n",
    "   return audio\n",
    "def append_audio(item_name, path, out_est, out_target, out_est_up, out_mix):\n",
    "   print(path)\n",
    "   source1_path = f\"{path}/{item_name}_source1hat_up.wav\"\n",
    "   source2_path = f\"{path}/{item_name}_source2hat_up.wav\"\n",
    "   if not os.path.isfile(source1_path) or not os.path.isfile(source2_path):\n",
    "      return\n",
    "\n",
    "   # Orig\n",
    "   source1_path = f\"{path}/{item_name}_source1.wav\"\n",
    "   source2_path = f\"{path}/{item_name}_source2.wav\"\n",
    "   out_target[item_name] = [parse_audio(source1_path), parse_audio(source2_path)]\n",
    "\n",
    "   source1_path = f\"{path}/{item_name}_source1hat.wav\"\n",
    "   source2_path = f\"{path}/{item_name}_source2hat.wav\"\n",
    "   out_est[item_name] = [parse_audio(source1_path), parse_audio(source2_path)]\n",
    "   \n",
    "   source1_path = f\"{path}/{item_name}_source1hat_up.wav\"\n",
    "   source2_path = f\"{path}/{item_name}_source2hat_up.wav\"\n",
    "   out_est_up[item_name] = [parse_audio(source1_path), parse_audio(source2_path)]\n",
    "\n",
    "   mix_path = f\"{path}/{item_name}_mix.wav\"\n",
    "   out_mix[item_name]=(parse_audio(mix_path))\n",
    "\n",
    "def read_all_audio(path, upsampled=False):\n",
    "   \n",
    "   audio_ids = []\n",
    "   out_target= {}\n",
    "   out_est = {}\n",
    "   out_est_upsampled = {}\n",
    "   out_mix = {}\n",
    "   all_files = os.listdir(path)\n",
    "   item_names = [file_name.split('_')[0] for file_name in all_files] \n",
    "   item_names = list(set(item_names))\n",
    "   \n",
    "   # with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "   for item_name in tqdm(item_names, total=len(item_names)):\n",
    "      append_audio(item_name, path, out_est, out_target, out_est_upsampled, out_mix)\n",
    "      \n",
    "   audio_ids =list(sorted(out_target.keys())) \n",
    "   out_target = list(dict(sorted(out_target.items())).values())\n",
    "   out_est= list(dict(sorted(out_est.items())).values())\n",
    "   out_est_upsampled= list(dict(sorted(out_est_upsampled.items())).values())\n",
    "   out_mix= list(dict(sorted(out_mix.items())).values())\n",
    "   return audio_ids, out_target, out_est,out_est_upsampled, out_mix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ant', 'ayy'), ('dog', 'arf')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = {\"dog\":\"arf\",\"ant\":\"ayy\"}\n",
    "sorted(test.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Separation(sb.Brain):\n",
    "   def compute_objectives(self, predictions, targets):\n",
    "        \"\"\"Computes the si-snr loss\"\"\"\n",
    "        return self.hparams.loss(targets, predictions)\n",
    "     \n",
    "   def get_metrics(self,audio_ids, targets, preds, mixtures,output_path):\n",
    "\n",
    "        # Create folders where to store audio\n",
    "      # save_file = os.path.join(output_path, \"test_results.csv\")\n",
    "      save_file = output_path\n",
    "\n",
    "        # Variable init\n",
    "\n",
    "      all_sdrs = []\n",
    "      all_sdrs_i = []\n",
    "      all_sisnrs = []\n",
    "      all_sisnrs_i = []\n",
    "      csv_columns = [\"snt_id\", \"sdr\", \"sdr_i\", \"si-snr\", \"si-snr_i\"]\n",
    "\n",
    "        \n",
    "      \n",
    "      with open(save_file, \"w\") as results_csv:\n",
    "         writer = csv.DictWriter(results_csv, fieldnames=csv_columns)\n",
    "         writer.writeheader()\n",
    "            \n",
    "         for audio_id, target, pred,mixture_ in tqdm(zip(audio_ids,targets, preds,mixtures), total=len(targets)):\n",
    "            target = torch.cat(\n",
    "            [target[i].unsqueeze(-1) for i in range(self.hparams.num_spks)],\n",
    "            dim=-1,\n",
    "        ).to(self.device)\n",
    "            \n",
    "            pred = torch.cat(\n",
    "            [pred[i].unsqueeze(-1) for i in range(self.hparams.num_spks)],\n",
    "            dim=-1,\n",
    "        ).to(self.device)\n",
    "            \n",
    "  \n",
    "            sisnr = self.compute_objectives(pred, target)\n",
    "            # COmpute SI-SNR Improvement\n",
    "            mixture_signal = torch.stack(\n",
    "               [mixture_] * self.hparams.num_spks, dim=-1\n",
    "            )\n",
    "            \n",
    "            arrs = [pred[0], target[0], mixture_signal[0]]\n",
    "            for item in arrs:\n",
    "               temp_item = (item!=np.inf).all() and (item!=np.NINF).all()\n",
    "               if not temp_item:\n",
    "                  continue\n",
    "            # if mixture_signal[0] == np.inf or mixture_signal[1] ==np.inf:\n",
    "            \n",
    "            mixture_signal = mixture_signal.to(target.device)\n",
    "            sisnr_baseline = self.compute_objectives(\n",
    "               mixture_signal, target\n",
    "            )\n",
    "            sisnr_i = sisnr.cpu().numpy() - sisnr_baseline.cpu().numpy()\n",
    "  # Compute SDR\n",
    "            sdr, _, _, _ = bss_eval_sources(\n",
    "               target[0].t(),\n",
    "               pred[0].t(),\n",
    "            )\n",
    "\n",
    "            sdr_baseline, _, _, _ = bss_eval_sources(\n",
    "               target[0].t(),\n",
    "               mixture_signal[0].t(),\n",
    "            )\n",
    "            \n",
    "            sdr = sdr.cpu().numpy()\n",
    "            sdr_baseline = sdr_baseline.cpu().numpy()\n",
    "           \n",
    "            sdr_i = sdr.mean() - sdr_baseline.mean()\n",
    "\n",
    "            # Saving on a csv file\n",
    "            row = {\n",
    "               \"snt_id\": audio_id,\n",
    "               \"sdr\": sdr.mean(),\n",
    "               \"sdr_i\": sdr_i,\n",
    "               \"si-snr\": -sisnr.item(),\n",
    "               \"si-snr_i\": -sisnr_i.item(),\n",
    "            }\n",
    "            writer.writerow(row)\n",
    "\n",
    "            # Metric Accumulation\n",
    "            all_sdrs.append(sdr.mean())\n",
    "            all_sdrs_i.append(sdr_i.mean())\n",
    "            all_sisnrs.append(-sisnr.item())\n",
    "            all_sisnrs_i.append(-sisnr_i.item())\n",
    "      # logger.info(\"Mean SISNR is {}\".format(np.array(all_sisnrs).mean()))\n",
    "      # logger.info(\"Mean SISNRi is {}\".format(np.array(all_sisnrs_i).mean()))\n",
    "      # logger.info(\"Mean SDR is {}\".format(np.array(all_sdrs).mean()))\n",
    "      # logger.info(\"Mean SDRi is {}\".format(np.array(all_sdrs_i).mean()))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading standard_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../sequential/separated\n",
      "../sequential/separated\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with standard_model | upsamle: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load hyperparameters file with command-line overrides\n",
    "hparams_file, run_opts, overrides = sb.parse_arguments([\"hyperparams.yaml\"])\n",
    "hparams_file = 'hyperparams.yaml'\n",
    "#    run_opts = {\"device\": \"cuda:0\"}\n",
    "with open(hparams_file) as fin:\n",
    "   hparams = load_hyperpyyaml(fin, overrides)\n",
    "   \n",
    "# Load pretrained model if pretrained_separator is present in the yaml\n",
    "if \"pretrained_separator\" in hparams:\n",
    "   # run_on_main(hparams[\"pretrained_separator\"].collect_files)\n",
    "   hparams[\"pretrained_separator\"].load_collected(\n",
    "      device=run_opts[\"device\"]\n",
    "   )\n",
    "# Brain class initialization\n",
    "separator = Separation(\n",
    "   modules=hparams[\"modules\"],\n",
    "   run_opts={\"device\": \"cuda\"},\n",
    "   hparams=hparams,\n",
    ")\n",
    "\n",
    "configs = [\"standard_model\"]\n",
    "# configs = [\"w_noise_speedperturb\",\"w_noise_wavedrop\"]\n",
    "# configs = [\"no_noise_speedperturb\"]\n",
    "UPSAMPLE= [False]\n",
    "# UPSAMPLE= [True]\n",
    "curr_aud =[]\n",
    "for config in configs:\n",
    "   print(f\"Reading {config}\")\n",
    "   # out = read_all_audio(f'results/{config}/audio_results')\n",
    "   # audio_ids,out_target, out_est, out_est_upsampled, out_mix = out\n",
    "   # audio_ids,out_target, out_est, out_est_upsampled, out_mix = read_all_audio(f'results/{config}/audio_results')\n",
    "   audio_ids,out_target, out_est, out_est_upsampled, out_mix = read_all_audio(f'../sequential/separated')\n",
    "   print(audio_ids)\n",
    "   \n",
    "   for is_upsample in UPSAMPLE:\n",
    "      if is_upsample:\n",
    "         out_target = out_est\n",
    "         out_est = out_est_upsampled   \n",
    "      # upsample = \"upsampled\" if upsample else \"not_upsampled\"\n",
    "      output_path = f\"results/{config}/{config}_{is_upsample}_test.csv\"\n",
    "      separator.get_metrics(audio_ids, out_target, out_est, out_mix, output_path=output_path)\n",
    "      print(f\"Done with {config} | upsamle: {is_upsample}\")\n",
    "# output_path = f\"results/epoch_30/true_test_result.csv\"\n",
    "# separator.get_metrics(audio_ids, out_target, out_est, out_mix, output_path=output_path)\n",
    "# separator.get_metrics(audio_ids, out_est, out_est_upsampled, out_mix, output_path=output_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieves the worst, median and best separation audio mixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------CONFIG: standard_model | upsampled: False\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "reduction operation 'argmin' not allowed for this dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39m# df.replace()\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m------------CONFIG: \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m}\u001b[39;00m\u001b[39m | upsampled: \u001b[39m\u001b[39m{\u001b[39;00mupsample\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m worst \u001b[39m=\u001b[39m get_worst(\u001b[39m'\u001b[39;49m\u001b[39msi-snr\u001b[39;49m\u001b[39m'\u001b[39;49m,df)\n\u001b[0;32m     39\u001b[0m median \u001b[39m=\u001b[39m get_median(\u001b[39m'\u001b[39m\u001b[39msi-snr\u001b[39m\u001b[39m'\u001b[39m,df)\n\u001b[0;32m     40\u001b[0m best \u001b[39m=\u001b[39m get_best(\u001b[39m'\u001b[39m\u001b[39msi-snr\u001b[39m\u001b[39m'\u001b[39m,df)\n",
      "Cell \u001b[1;32mIn[24], line 6\u001b[0m, in \u001b[0;36mget_worst\u001b[1;34m(column, df)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_worst\u001b[39m(column,df):\n\u001b[1;32m----> 6\u001b[0m    \u001b[39mreturn\u001b[39;00m df\u001b[39m.\u001b[39mloc[df[column]\u001b[39m.\u001b[39;49midxmin()]\n",
      "File \u001b[1;32mc:\\Users\\Arian\\.conda\\envs\\inference\\lib\\site-packages\\pandas\\core\\series.py:2461\u001b[0m, in \u001b[0;36mSeries.idxmin\u001b[1;34m(self, axis, skipna, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2397\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2398\u001b[0m \u001b[39mReturn the row label of the minimum value.\u001b[39;00m\n\u001b[0;32m   2399\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2457\u001b[0m \u001b[39mnan\u001b[39;00m\n\u001b[0;32m   2458\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2459\u001b[0m \u001b[39m# error: Argument 1 to \"argmin\" of \"IndexOpsMixin\" has incompatible type \"Union\u001b[39;00m\n\u001b[0;32m   2460\u001b[0m \u001b[39m# [int, Literal['index', 'columns']]\"; expected \"Optional[int]\"\u001b[39;00m\n\u001b[1;32m-> 2461\u001b[0m i \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margmin(axis, skipna, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   2462\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[0;32m   2463\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mnan\n",
      "File \u001b[1;32mc:\\Users\\Arian\\.conda\\envs\\inference\\lib\\site-packages\\pandas\\core\\base.py:742\u001b[0m, in \u001b[0;36mIndexOpsMixin.argmin\u001b[1;34m(self, axis, skipna, *args, **kwargs)\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[39mreturn\u001b[39;00m delegate\u001b[39m.\u001b[39margmin()\n\u001b[0;32m    739\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    740\u001b[0m     \u001b[39m# error: Incompatible return value type (got \"Union[int, ndarray]\", expected\u001b[39;00m\n\u001b[0;32m    741\u001b[0m     \u001b[39m# \"int\")\u001b[39;00m\n\u001b[1;32m--> 742\u001b[0m     \u001b[39mreturn\u001b[39;00m nanops\u001b[39m.\u001b[39;49mnanargmin(  \u001b[39m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[0;32m    743\u001b[0m         delegate, skipna\u001b[39m=\u001b[39;49mskipna\n\u001b[0;32m    744\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Arian\\.conda\\envs\\inference\\lib\\site-packages\\pandas\\core\\nanops.py:91\u001b[0m, in \u001b[0;36mdisallow.__call__.<locals>._f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck(obj) \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m obj_iter):\n\u001b[0;32m     90\u001b[0m     f_name \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mnan\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 91\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m     92\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreduction operation \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mf_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not allowed for this dtype\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[0;32m     94\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(invalid\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: reduction operation 'argmin' not allowed for this dtype"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "# Get worst\n",
    "def get_worst(column,df):\n",
    "   return df.loc[df[column].idxmin()]\n",
    "# Get best \n",
    "def get_best(column,df):\n",
    "   return df.loc[df[column].idxmax()]\n",
    "# Get Median\n",
    "def get_median(column,df):\n",
    "   df.loc[df[column]== df[column].median()]\n",
    "   df.sort_values(by=column, inplace=True)\n",
    "   return df[df[column] < df[column].median()].iloc[-1]\n",
    "\n",
    "\n",
    "def print_audio_info(stat, df):\n",
    "   print(f\"status: {stat} | sdr: {df['sdr']} | sdr_i: {df['sdr_i']} | si-snr: {df['si-snr']} | si-snr_i: {df['si-snr_i']}\")\n",
    "\n",
    "def write_demo_audio(src_path,dest_path):\n",
    "   shutil.copy(src_path, dest_path)\n",
    "\n",
    "def write_image(dest_path):\n",
    "   pass\n",
    "   \n",
    "status = ['Worst', 'Median', 'Best']\n",
    "\n",
    "configs = [\"standard_model\", \"no_noise_speedperturb\",\"w_noise_speedperturb\", \"w_noise_wavedrop\"]\n",
    "# configs = [\"no_noise_speedperturb\"]\n",
    "for config in configs:\n",
    "   for upsample in [False, True]:\n",
    "      df = pd.read_csv(f'results/{config}/{config}_{upsample}_test.csv')\n",
    "      df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "   # drop rows with NaNs\n",
    "      df.dropna(inplace=True)\n",
    "      # df.replace()\n",
    "      print(f\"------------CONFIG: {config} | upsampled: {upsample}\")\n",
    "      worst = get_worst('si-snr',df)\n",
    "      median = get_median('si-snr',df)\n",
    "      best = get_best('si-snr',df)\n",
    "      print_audio_info(status[0], worst)\n",
    "      print_audio_info(status[1], median)\n",
    "      print_audio_info(status[2], best)\n",
    "\n",
    "      mean_values = df.loc[:, df.columns != 'snt_id'].mean()\n",
    "      print(f'mean_values {mean_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snt_id</th>\n",
       "      <th>sdr</th>\n",
       "      <th>sdr_i</th>\n",
       "      <th>si-snr</th>\n",
       "      <th>si-snr_i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>item0</td>\n",
       "      <td>8.709541</td>\n",
       "      <td>12.099934</td>\n",
       "      <td>7.763442</td>\n",
       "      <td>11.358368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>item1</td>\n",
       "      <td>-1.304001</td>\n",
       "      <td>-0.404314</td>\n",
       "      <td>-2.374741</td>\n",
       "      <td>-1.409151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>item10</td>\n",
       "      <td>10.374760</td>\n",
       "      <td>11.388518</td>\n",
       "      <td>9.455255</td>\n",
       "      <td>10.703505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>item100</td>\n",
       "      <td>8.781614</td>\n",
       "      <td>10.849915</td>\n",
       "      <td>7.796484</td>\n",
       "      <td>10.323214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>item1000</td>\n",
       "      <td>8.809902</td>\n",
       "      <td>9.547372</td>\n",
       "      <td>8.316668</td>\n",
       "      <td>9.167922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     snt_id        sdr      sdr_i    si-snr   si-snr_i\n",
       "0     item0   8.709541  12.099934  7.763442  11.358368\n",
       "1     item1  -1.304001  -0.404314 -2.374741  -1.409151\n",
       "2    item10  10.374760  11.388518  9.455255  10.703505\n",
       "3   item100   8.781614  10.849915  7.796484  10.323214\n",
       "4  item1000   8.809902   9.547372  8.316668   9.167922"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sort_values(by='snt_id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import librosa\n",
    "\n",
    "# # Load the original audio signal\n",
    "# audio_file_orig = 'original_audio.wav'\n",
    "# y_orig, sr_orig = librosa.load(audio_file_orig)\n",
    "\n",
    "# # Load the upsampled audio signal\n",
    "# audio_file_upsamp = 'upsampled_audio.wav'\n",
    "# y_upsamp, sr_upsamp = librosa.load(audio_file_upsamp)\n",
    "\n",
    "# # Compute the SNR of the upsampled audio signal compared to the original signal\n",
    "# diff = np.sum((y_orig - y_upsamp)**2)\n",
    "# snr = 10 * np.log10(np.sum(y_orig**2) / diff)\n",
    "\n",
    "# print('SNR: {:.2f} dB'.format(snr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectral centroid difference: -215.47\n",
      "Spectral rolloff difference: -267.58\n",
      "Spectral centroid std difference: -97.98\n",
      "Spectral rolloff std difference: -108.21\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "# Load the original audio signal\n",
    "audio_file_orig = 'original_audio.wav'\n",
    "y_orig, sr_orig = librosa.load(audio_file_orig)\n",
    "\n",
    "# Load the upsampled audio signal\n",
    "audio_file_upsamp = 'upsampled_audio.wav'\n",
    "y_upsamp, sr_upsamp = librosa.load(audio_file_upsamp)\n",
    "\n",
    "# Compute the spectral centroid and rolloff of the original and upsampled audio signals\n",
    "spec_centroid_orig = librosa.feature.spectral_centroid(y_orig, sr=sr_orig)[0]\n",
    "spec_rolloff_orig = librosa.feature.spectral_rolloff(y_orig, sr=sr_orig)[0]\n",
    "\n",
    "spec_centroid_upsamp = librosa.feature.spectral_centroid(y_upsamp, sr=sr_upsamp)[0]\n",
    "spec_rolloff_upsamp = librosa.feature.spectral_rolloff(y_upsamp, sr=sr_upsamp)[0]\n",
    "\n",
    "# Compute the mean and standard deviation of the spectral features\n",
    "centroid_diff = spec_centroid_orig.mean() - spec_centroid_upsamp.mean()\n",
    "rolloff_diff = spec_rolloff_orig.mean() - spec_rolloff_upsamp.mean()\n",
    "\n",
    "centroid_std_diff = spec_centroid_orig.std() - spec_centroid_upsamp.std()\n",
    "rolloff_std_diff = spec_rolloff_orig.std() - spec_rolloff_upsamp.std()\n",
    "\n",
    "# Print the results\n",
    "print('Spectral centroid difference: {:.2f}'.format(centroid_diff))\n",
    "print('Spectral rolloff difference: {:.2f}'.format(rolloff_diff))\n",
    "\n",
    "print('Spectral centroid std difference: {:.2f}'.format(centroid_std_diff))\n",
    "print('Spectral rolloff std difference: {:.2f}'.format(rolloff_std_diff))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22050, 22050)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr_upsamp, sr_orig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
