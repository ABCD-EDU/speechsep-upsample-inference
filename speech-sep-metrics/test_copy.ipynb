{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import speechbrain as sb\n",
    "import speechbrain.nnet.schedulers as schedulers\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "import csv\n",
    "device = torch.device('cuda')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# from mir_eval.separation import bss_eval_sources\n",
    "from speechbrain.dataio.dataio import read_audio\n",
    "from fast_bss_eval import bss_eval_sources\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 17275/21000 [07:03<01:31, 40.84it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m    out_est \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out_est\u001b[39m.\u001b[39mvalues())\n\u001b[0;32m     42\u001b[0m    \u001b[39mreturn\u001b[39;00m audio_ids, out_target, out_est, out_mix\n\u001b[1;32m---> 44\u001b[0m audio_ids,out_target, out_est, out_mix \u001b[39m=\u001b[39m read_all_audio(\u001b[39m'\u001b[39;49m\u001b[39mresults/audio_results\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[1;32mIn[17], line 36\u001b[0m, in \u001b[0;36mread_all_audio\u001b[1;34m(path, upsampled)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mif\u001b[39;00m upsampled \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m file_name \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mhat\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m file_name:\n\u001b[0;32m     35\u001b[0m    \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m audio, sr_ \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39;49mload(filepath, sr\u001b[39m=\u001b[39;49m\u001b[39m8000\u001b[39;49m)\n\u001b[0;32m     37\u001b[0m audio \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(audio)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m     38\u001b[0m append_audio(upsampled, item_name, file_name, out_est, out_target,out_mix, audio)\n",
      "File \u001b[1;32mc:\\Users\\Arian\\.conda\\envs\\inference\\lib\\site-packages\\librosa\\core\\audio.py:172\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    169\u001b[0m     y \u001b[39m=\u001b[39m to_mono(y)\n\u001b[0;32m    171\u001b[0m \u001b[39mif\u001b[39;00m sr \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m     y \u001b[39m=\u001b[39m resample(y, sr_native, sr, res_type\u001b[39m=\u001b[39;49mres_type)\n\u001b[0;32m    174\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     sr \u001b[39m=\u001b[39m sr_native\n",
      "File \u001b[1;32mc:\\Users\\Arian\\.conda\\envs\\inference\\lib\\site-packages\\librosa\\core\\audio.py:584\u001b[0m, in \u001b[0;36mresample\u001b[1;34m(y, orig_sr, target_sr, res_type, fix, scale, **kwargs)\u001b[0m\n\u001b[0;32m    582\u001b[0m     y_hat \u001b[39m=\u001b[39m samplerate\u001b[39m.\u001b[39mresample(y\u001b[39m.\u001b[39mT, ratio, converter_type\u001b[39m=\u001b[39mres_type)\u001b[39m.\u001b[39mT\n\u001b[0;32m    583\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m     y_hat \u001b[39m=\u001b[39m resampy\u001b[39m.\u001b[39;49mresample(y, orig_sr, target_sr, \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49mres_type, axis\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    586\u001b[0m \u001b[39mif\u001b[39;00m fix:\n\u001b[0;32m    587\u001b[0m     y_hat \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mfix_length(y_hat, n_samples, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Arian\\.conda\\envs\\inference\\lib\\site-packages\\resampy\\core.py:168\u001b[0m, in \u001b[0;36mresample\u001b[1;34m(x, sr_orig, sr_new, axis, filter, parallel, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m         resample_f_s(\n\u001b[0;32m    159\u001b[0m             x\u001b[39m.\u001b[39mswapaxes(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, axis),\n\u001b[0;32m    160\u001b[0m             t_out,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m             y\u001b[39m.\u001b[39mswapaxes(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, axis),\n\u001b[0;32m    166\u001b[0m         )\n\u001b[0;32m    167\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     resample_f_s(\n\u001b[0;32m    169\u001b[0m         x\u001b[39m.\u001b[39;49mswapaxes(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, axis),\n\u001b[0;32m    170\u001b[0m         t_out,\n\u001b[0;32m    171\u001b[0m         interp_win,\n\u001b[0;32m    172\u001b[0m         interp_delta,\n\u001b[0;32m    173\u001b[0m         precision,\n\u001b[0;32m    174\u001b[0m         scale,\n\u001b[0;32m    175\u001b[0m         y\u001b[39m.\u001b[39;49mswapaxes(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, axis),\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\Arian\\.conda\\envs\\inference\\lib\\site-packages\\numba\\np\\ufunc\\gufunc.py:192\u001b[0m, in \u001b[0;36mGUFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd(sig)\n\u001b[0;32m    191\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_ufunc()\n\u001b[1;32m--> 192\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mufunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "def append_audio(upsampled, item_name, file_name, out_est, out_target,out_mix, audio):\n",
    "   if upsampled:\n",
    "      if \"source\" in file_name and \"hat_up\" not in file_name :\n",
    "         out_target[item_name] =out_target[item_name]+[(audio)] if item_name in out_target else [audio]\n",
    "         return\n",
    "      elif \"hat_up\" in file_name:\n",
    "         out_est[item_name] =out_est[item_name]+[(audio)] if item_name in out_est else [audio]\n",
    "         return\n",
    "      elif \"mix\" in file_name:\n",
    "         out_mix.append(audio)\n",
    "   if not upsampled:\n",
    "      if \"source\" in file_name and \"hat\" not in file_name:\n",
    "         out_target[item_name] =out_target[item_name]+[(audio)] if item_name in out_target else [audio]\n",
    "         return\n",
    "      elif \"hat\" in file_name and \"hat_up\" not in file_name:\n",
    "         out_est[item_name] = out_est[item_name]+[(audio)] if item_name in out_est  else [audio]\n",
    "         return\n",
    "      elif \"mix\" in file_name:\n",
    "         out_mix.append(audio)\n",
    "         \n",
    "\n",
    "def read_all_audio(path, upsampled=False):\n",
    "   \n",
    "   audio_ids = []\n",
    "   out_target_orig = {}\n",
    "   out_target_pred = {}\n",
    "   out_est = {}\n",
    "   out_upsampled = {}\n",
    "   out_mix = []\n",
    "   all_files = os.listdir(path)[:70]\n",
    "   all_files = os.listdir(path)\n",
    "   for file_name in tqdm(all_files, total=len(all_files)) :\n",
    "      filepath = os.path.join(path, file_name)\n",
    "      item_name = file_name.split('_')[0]\n",
    "      if upsampled and \"source\" in file_name and \"hat\" not in file_name:\n",
    "         continue\n",
    "      audio, sr_ = librosa.load(filepath, sr=8000)\n",
    "      audio = torch.from_numpy(audio).unsqueeze(0)\n",
    "      append_audio(upsampled, item_name, file_name, out_est, out_target,out_mix, audio)\n",
    "   audio_ids = list(out_target.keys()) \n",
    "   out_target = list(out_target.values())\n",
    "   out_est = list(out_est.values())\n",
    "   return audio_ids, out_target, out_est, out_mix\n",
    "\n",
    "audio_ids,out_target, out_est, out_mix = read_all_audio('results/audio_results', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_audio, sr_orig = librosa.load('results/audio_results/item0_source1.wav',)\n",
    "# hat_audio, sr_hat = librosa.load('results/audio_results/item0_source1hat.wav',8000)\n",
    "# hat_up_audio, sr_hat_up = librosa.load('results/audio_results/item0_source1hat_up.wav',)\n",
    "# orig_audio.shape, sr_orig, hat_audio.shape, sr_hat, hat_up_audio.shape, sr_hat_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3000, 3000, 3000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audio_ids),len(out_target), len(out_est) , len(out_mix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('item0',\n",
       " [tensor([[ 0.0006,  0.0009,  0.0007,  ...,  0.0009,  0.0001, -0.0002]]),\n",
       "  tensor([[-0.0013, -0.0022, -0.0013,  ..., -0.0037, -0.0023, -0.0005]])],\n",
       " [tensor([[-0.0033, -0.0048,  0.0008,  ...,  0.0029, -0.0053,  0.0002]]),\n",
       "  tensor([[-0.0056, -0.0062, -0.0009,  ..., -0.0041, -0.0057, -0.0012]])],\n",
       " tensor([[-0.0019, -0.0030, -0.0024,  ..., -0.0047, -0.0019,  0.0015]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_ids[0], out_target[0], out_est[0], out_mix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Separation(sb.Brain):\n",
    "   def compute_objectives(self, predictions, targets):\n",
    "        \"\"\"Computes the si-snr loss\"\"\"\n",
    "        return self.hparams.loss(targets, predictions)\n",
    "     \n",
    "   def get_metrics(self,audio_ids, targets, preds, mixtures):\n",
    "\n",
    "        # Create folders where to store audio\n",
    "      save_file = os.path.join(self.hparams.output_folder, \"test_results.csv\")\n",
    "\n",
    "        # Variable init\n",
    "\n",
    "      all_sdrs = []\n",
    "      all_sdrs_i = []\n",
    "      all_sisnrs = []\n",
    "      all_sisnrs_i = []\n",
    "      csv_columns = [\"snt_id\", \"sdr\", \"sdr_i\", \"si-snr\", \"si-snr_i\"]\n",
    "\n",
    "        \n",
    "      \n",
    "      with open(save_file, \"w\") as results_csv:\n",
    "         writer = csv.DictWriter(results_csv, fieldnames=csv_columns)\n",
    "         writer.writeheader()\n",
    "            \n",
    "         for audio_id, target, pred,mixture_ in tqdm(zip(audio_ids,targets, preds,mixtures), total=len(targets)):\n",
    "            target = torch.cat(\n",
    "            [target[i].unsqueeze(-1) for i in range(self.hparams.num_spks)],\n",
    "            dim=-1,\n",
    "        ).to(self.device)\n",
    "            \n",
    "            pred = torch.cat(\n",
    "            [pred[i].unsqueeze(-1) for i in range(self.hparams.num_spks)],\n",
    "            dim=-1,\n",
    "        ).to(self.device)\n",
    "  \n",
    "            sisnr = self.compute_objectives(pred, target)\n",
    "            # COmpute SI-SNR Improvement\n",
    "            mixture_signal = torch.stack(\n",
    "               [mixture_] * self.hparams.num_spks, dim=-1\n",
    "            )\n",
    "            mixture_signal = mixture_signal.to(target.device)\n",
    "            sisnr_baseline = self.compute_objectives(\n",
    "               mixture_signal, target\n",
    "            )\n",
    "            sisnr_i = sisnr.cpu().numpy() - sisnr_baseline.cpu().numpy()\n",
    "  # Compute SDR\n",
    "            sdr, _, _, _ = bss_eval_sources(\n",
    "               target[0].t(),\n",
    "               pred[0].t(),\n",
    "            )\n",
    "\n",
    "            sdr_baseline, _, _, _ = bss_eval_sources(\n",
    "               target[0].t(),\n",
    "               mixture_signal[0].t(),\n",
    "            )\n",
    "            \n",
    "            sdr = sdr.cpu().numpy()\n",
    "            sdr_baseline = sdr_baseline.cpu().numpy()\n",
    "           \n",
    "            sdr_i = sdr.mean() - sdr_baseline.mean()\n",
    "\n",
    "            # Saving on a csv file\n",
    "            row = {\n",
    "               \"snt_id\": audio_id,\n",
    "               \"sdr\": sdr.mean(),\n",
    "               \"sdr_i\": sdr_i,\n",
    "               \"si-snr\": -sisnr.item(),\n",
    "               \"si-snr_i\": -sisnr_i.item(),\n",
    "            }\n",
    "            writer.writerow(row)\n",
    "\n",
    "            # Metric Accumulation\n",
    "            all_sdrs.append(sdr.mean())\n",
    "            all_sdrs_i.append(sdr_i.mean())\n",
    "            all_sisnrs.append(-sisnr.item())\n",
    "            all_sisnrs_i.append(-sisnr_i.item())\n",
    "      # logger.info(\"Mean SISNR is {}\".format(np.array(all_sisnrs).mean()))\n",
    "      # logger.info(\"Mean SISNRi is {}\".format(np.array(all_sisnrs_i).mean()))\n",
    "      # logger.info(\"Mean SDR is {}\".format(np.array(all_sdrs).mean()))\n",
    "      # logger.info(\"Mean SDRi is {}\".format(np.array(all_sdrs_i).mean()))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 35160]), torch.Size([1, 35160]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_target[0][0].shape, out_est[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "100%|██████████| 3000/3000 [02:41<00:00, 18.63it/s]\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    # Load hyperparameters file with command-line overrides\n",
    "   hparams_file, run_opts, overrides = sb.parse_arguments([\"hyperparams.yaml\"])\n",
    "   hparams_file = 'hyperparams.yaml'\n",
    "#    run_opts = {\"device\": \"cuda:0\"}\n",
    "   with open(hparams_file) as fin:\n",
    "      hparams = load_hyperpyyaml(fin, overrides)\n",
    "        \n",
    "      # Load pretrained model if pretrained_separator is present in the yaml\n",
    "   if \"pretrained_separator\" in hparams:\n",
    "       # run_on_main(hparams[\"pretrained_separator\"].collect_files)\n",
    "       hparams[\"pretrained_separator\"].load_collected(\n",
    "            device=run_opts[\"device\"]\n",
    "       )\n",
    "   # Brain class initialization\n",
    "   separator = Separation(\n",
    "        modules=hparams[\"modules\"],\n",
    "        run_opts={\"device\": \"cuda\"},\n",
    "        hparams=hparams,\n",
    "    )\n",
    "   separator.get_metrics(audio_ids, out_target, out_est, out_mix)\n",
    "run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieves the worst, median and best separation audio mixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------CONFIG: no_noise_speedperturb\n",
      "status: Worst | sdr: 7.472182 | sdr_i: 0.14493132 | si-snr: 4.14491081237793 | si-snr_i: -2.8795289993286133\n",
      "status: Median | sdr: 21.159967 | sdr_i: 20.366205 | si-snr: 14.19557285308838 | si-snr_i: 13.583800315856934\n",
      "status: Best | sdr: 27.271698 | sdr_i: 25.772282 | si-snr: 20.748476028442383 | si-snr_i: 19.38570022583008\n",
      "mean_values sdr         20.395723\n",
      "sdr_i       18.038166\n",
      "si-snr      13.974293\n",
      "si-snr_i    11.997022\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Get worst\n",
    "def get_worst(column,df):\n",
    "   return df.loc[df[column].idxmin()]\n",
    "# Get best \n",
    "def get_best(column,df):\n",
    "   return df.loc[df[column].idxmax()]\n",
    "# Get Median\n",
    "def get_median(column,df):\n",
    "   df.loc[df[column]== df[column].median()]\n",
    "   df.sort_values(by=column, inplace=True)\n",
    "   return df[df[column] < df[column].median()].iloc[-1]\n",
    "\n",
    "\n",
    "def print_audio_info(stat, df):\n",
    "   print(f\"status: {stat} | sdr: {df['sdr']} | sdr_i: {df['sdr_i']} | si-snr: {df['si-snr']} | si-snr_i: {df['si-snr_i']}\")\n",
    "status = ['Worst', 'Median', 'Best']\n",
    "\n",
    "# configs = [\"no_noise\", \"no_noise_speedperturb\",\"w_noise_speedperturb\", \"w_noise_wavedrop\"]\n",
    "configs = [\"no_noise_speedperturb\"]\n",
    "for config in configs:\n",
    "   df = pd.read_csv(f'results/upsampled_metrics/{config}.csv')\n",
    "   df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# drop rows with NaNs\n",
    "   df.dropna(inplace=True)\n",
    "   # df.replace()\n",
    "   print(f\"------------CONFIG: {config}\")\n",
    "   print_audio_info(status[0], get_worst('si-snr',df))\n",
    "   print_audio_info(status[1], get_median('si-snr',df))\n",
    "   print_audio_info(status[2], get_best('si-snr',df))\n",
    "   mean_values = df.loc[:, df.columns != 'snt_id'].mean()\n",
    "   print(f'mean_values {mean_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snt_id</th>\n",
       "      <th>sdr</th>\n",
       "      <th>sdr_i</th>\n",
       "      <th>si-snr</th>\n",
       "      <th>si-snr_i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>item0</td>\n",
       "      <td>27.312160</td>\n",
       "      <td>26.274744</td>\n",
       "      <td>16.086637</td>\n",
       "      <td>15.195269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>item1</td>\n",
       "      <td>20.704279</td>\n",
       "      <td>15.591076</td>\n",
       "      <td>11.947648</td>\n",
       "      <td>7.364430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>item10</td>\n",
       "      <td>27.709038</td>\n",
       "      <td>25.243404</td>\n",
       "      <td>15.365202</td>\n",
       "      <td>13.305538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>item100</td>\n",
       "      <td>20.437866</td>\n",
       "      <td>17.912374</td>\n",
       "      <td>14.745030</td>\n",
       "      <td>12.808769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>item1000</td>\n",
       "      <td>22.681060</td>\n",
       "      <td>20.429329</td>\n",
       "      <td>15.974695</td>\n",
       "      <td>13.866444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        snt_id        sdr      sdr_i     si-snr   si-snr_i\n",
       "0        item0  27.312160  26.274744  16.086637  15.195269\n",
       "1111     item1  20.704279  15.591076  11.947648   7.364430\n",
       "111     item10  27.709038  25.243404  15.365202  13.305538\n",
       "11     item100  20.437866  17.912374  14.745030  12.808769\n",
       "1     item1000  22.681060  20.429329  15.974695  13.866444"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sort_values(by='snt_id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR: 16.21 dB\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import librosa\n",
    "\n",
    "# # Load the original audio signal\n",
    "# audio_file_orig = 'original_audio.wav'\n",
    "# y_orig, sr_orig = librosa.load(audio_file_orig)\n",
    "\n",
    "# # Load the upsampled audio signal\n",
    "# audio_file_upsamp = 'upsampled_audio.wav'\n",
    "# y_upsamp, sr_upsamp = librosa.load(audio_file_upsamp)\n",
    "\n",
    "# # Compute the SNR of the upsampled audio signal compared to the original signal\n",
    "# diff = np.sum((y_orig - y_upsamp)**2)\n",
    "# snr = 10 * np.log10(np.sum(y_orig**2) / diff)\n",
    "\n",
    "# print('SNR: {:.2f} dB'.format(snr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectral centroid difference: -215.47\n",
      "Spectral rolloff difference: -267.58\n",
      "Spectral centroid std difference: -97.98\n",
      "Spectral rolloff std difference: -108.21\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "# Load the original audio signal\n",
    "audio_file_orig = 'original_audio.wav'\n",
    "y_orig, sr_orig = librosa.load(audio_file_orig)\n",
    "\n",
    "# Load the upsampled audio signal\n",
    "audio_file_upsamp = 'upsampled_audio.wav'\n",
    "y_upsamp, sr_upsamp = librosa.load(audio_file_upsamp)\n",
    "\n",
    "# Compute the spectral centroid and rolloff of the original and upsampled audio signals\n",
    "spec_centroid_orig = librosa.feature.spectral_centroid(y_orig, sr=sr_orig)[0]\n",
    "spec_rolloff_orig = librosa.feature.spectral_rolloff(y_orig, sr=sr_orig)[0]\n",
    "\n",
    "spec_centroid_upsamp = librosa.feature.spectral_centroid(y_upsamp, sr=sr_upsamp)[0]\n",
    "spec_rolloff_upsamp = librosa.feature.spectral_rolloff(y_upsamp, sr=sr_upsamp)[0]\n",
    "\n",
    "# Compute the mean and standard deviation of the spectral features\n",
    "centroid_diff = spec_centroid_orig.mean() - spec_centroid_upsamp.mean()\n",
    "rolloff_diff = spec_rolloff_orig.mean() - spec_rolloff_upsamp.mean()\n",
    "\n",
    "centroid_std_diff = spec_centroid_orig.std() - spec_centroid_upsamp.std()\n",
    "rolloff_std_diff = spec_rolloff_orig.std() - spec_rolloff_upsamp.std()\n",
    "\n",
    "# Print the results\n",
    "print('Spectral centroid difference: {:.2f}'.format(centroid_diff))\n",
    "print('Spectral rolloff difference: {:.2f}'.format(rolloff_diff))\n",
    "\n",
    "print('Spectral centroid std difference: {:.2f}'.format(centroid_std_diff))\n",
    "print('Spectral rolloff std difference: {:.2f}'.format(rolloff_std_diff))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22050, 22050)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr_upsamp, sr_orig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
